#!/usr/bin/env python3
import os
import random
import requests
import feedparser
from datetime import datetime

FEISHU_WEBHOOK = os.environ.get("FEISHU_WEBHOOK", "").strip()
DRY_RUN = os.getenv("DRY_RUN") == "1"

if not FEISHU_WEBHOOK and not DRY_RUN:
    raise SystemExit("Missing env FEISHU_WEBHOOK")

CHENGDU_LAT = 30.5728
CHENGDU_LON = 104.0668

TRENDING_FEEDS = [
    ("ğŸ”¥ HN", "https://hnrss.org/frontpage"),
    ("ğŸ’» TechCrunch", "https://techcrunch.com/feed/"),
    ("ğŸ¤– VentureBeat", "https://venturebeat.com/category/ai/feed/"),
    ("âš¡ Ars Technica", "https://feeds.arstechnica.com/arstechnica/technology-lab"),
    ("ğŸ“± The Verge", "https://www.theverge.com/rss/index.xml"),
    ("ğŸ”¬ MIT Tech", "https://www.technologyreview.com/feed/"),
]

DAY_NAMES = {
    0: "å¿™ Day",
    1: "å»æ­» Day",
    2: "æœªæ­» Day",
    3: "å—æ­» Day",
    4: "ç¦æ¥ Day",
    5: "æ´’è„± Day",
    6: "ä¸§ Day",
}
WEEKDAY_CN = {0:"å‘¨ä¸€",1:"å‘¨äºŒ",2:"å‘¨ä¸‰",3:"å‘¨å››",4:"å‘¨äº”",5:"å‘¨å…­",6:"å‘¨æ—¥"}

MAINLINE_POOL = {
    0: ["å¿™ Dayï¼šä½ ä»¬ä¸Šç­ï¼Œæˆ‘è´Ÿè´£å¯çˆ±å’Œæ’­æŠ¥ã€‚", "å¿™ Dayï¼šå…ˆä¸Šç­ï¼Œå†æ‘¸çŒ«ï¼ˆæˆ‘ï¼‰ã€‚"],
    1: ["å»æ­» Dayï¼šæˆ‘ä¸è¯„ä»·ï¼Œæˆ‘åªæƒ³åƒç½å¤´ã€‚", "å»æ­» Dayï¼šä¿æŒå‘¼å¸ï¼Œä¿æŒçŒ«ç²®é¢„ç®—ã€‚"],
    2: ["æœªæ­» Dayï¼šåšæŒä½ï¼ç¦»ç¦æ¥ Day æ›´è¿‘ä¸€æ­¥ã€‚", "æœªæ­» Dayï¼šå¼Ÿå¼Ÿå…è®¸ä½ ä»¬å–˜ä¸€å£æ°”å†å·ã€‚"],
    3: ["å—æ­» Dayï¼šå¿«åˆ°å‘¨äº”äº†ï¼Œåˆ«å€’ä¸‹ã€‚", "å—æ­» Dayï¼šæˆ‘å…ˆæ›¿ä½ ä»¬å¹æ°”â€”â€”å”‰ã€‚"],
    4: ["ç¦æ¥ Dayï¼šå‘¨æœ«çš„å‘³é“æˆ‘éƒ½é—»åˆ°äº†ã€‚", "ç¦æ¥ Dayï¼šä»Šå¤©é€‚åˆå·å·å¼€å¿ƒä¸€ä¸‹ã€‚"],
    5: ["æ´’è„± Dayï¼šä½ ä»¬ä¼‘æ¯ï¼Œæˆ‘ä¹Ÿèººå¹³å¹²é¥­ã€‚", "æ´’è„± Dayï¼šæ”¾ä¸‹æ‰‹æœºï¼Œæ‘¸æ‘¸çŒ«ï¼ˆæˆ‘ï¼‰ã€‚"],
    6: ["ä¸§ Dayï¼šå…è®¸ä¸§ï¼Œä½†ä¸è®¸é¥¿ç€ï¼ˆä¹Ÿä¸è®¸å¿˜äº†ç»™æˆ‘åŠ é¤ï¼‰ã€‚", "ä¸§ Dayï¼šæˆ‘é™ªä½ ä»¬å‘å‘†äº”åˆ†é’Ÿï¼Œç„¶åç»§ç»­æ´»ç€ã€‚"],
}

ASIDES = [
    "ï¼ˆæ–°é—»æ˜¯å¼æ¥çš„ï¼Œä½†çŒ«ç²®æ˜¯è¦ä½ ä»¬æŒ£çš„ã€‚ï¼‰",
    "ï¼ˆæ‘¸çŒ«èƒ½æå‡ç”Ÿäº§åŠ›ï¼ŒçœŸçš„ã€‚ï¼‰",
    "ï¼ˆæˆ‘åˆšåˆšä¼¸äº†ä¸ªæ‡’è…°ï¼šä»Šæ—¥çŠ¶æ€æ»¡åˆ†ã€‚ï¼‰",
    "ï¼ˆä½ ä»¬è®¤çœŸå·¥ä½œï¼Œæˆ‘è®¤çœŸå¯çˆ±ã€‚ï¼‰",
]

def fetch_weather_chengdu():
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": CHENGDU_LAT,
        "longitude": CHENGDU_LON,
        "daily": "weathercode,temperature_2m_max,temperature_2m_min,precipitation_probability_max",
        "timezone": "Asia/Shanghai",
        "forecast_days": 1,
    }
    r = requests.get(url, params=params, timeout=20)
    r.raise_for_status()
    data = r.json()["daily"]
    tmax = data["temperature_2m_max"][0]
    tmin = data["temperature_2m_min"][0]
    pop  = data["precipitation_probability_max"][0]
    code = data["weathercode"][0]
    code_map = {
        0: "æ™´", 1: "å¤§è‡´æ™´æœ—", 2: "å¤šäº‘", 3: "é˜´",
        45: "é›¾", 48: "é›¾å‡‡",
        51: "æ¯›æ¯›é›¨", 53: "æ¯›æ¯›é›¨", 55: "æ¯›æ¯›é›¨",
        61: "å°é›¨", 63: "ä¸­é›¨", 65: "å¤§é›¨",
        71: "å°é›ª", 73: "ä¸­é›ª", 75: "å¤§é›ª",
        80: "é˜µé›¨", 81: "é˜µé›¨", 82: "å¼ºé˜µé›¨",
        95: "é›·æš´",
    }
    desc = code_map.get(code, f"å¤©æ°”ä»£ç  {code}")
    return desc, tmin, tmax, pop

def _norm_title(t: str) -> str:
    return " ".join((t or "").lower().split())

def fetch_international_trending(limit=3):
    headers = {"User-Agent": "DidiDailyBriefingBot/1.0 (+https://github.com/)"}
    items = []
    seen_title = set()

    bad_phrases = [
        "self-promotion",
        "weekly thread",
        "monthly thread",
        "daily thread",
        "who's hiring",
        "who is hiring",
        "hiring thread",
        "who wants to be hired",
        "jobs thread",
        "ask hn: who is hiring",
        "ask hn: who wants to be hired",
    ]

    for section, url in TRENDING_FEEDS:
        if len(items) >= limit:
            break
        try:
            resp = requests.get(url, headers=headers, timeout=20)
            resp.raise_for_status()
            d = feedparser.parse(resp.content)
            if not d.entries:
                continue

            picked = None
            for e in d.entries[:10]:
                title = (e.get("title") or "").strip()
                link  = (e.get("link") or "").strip()
                if not title or not link:
                    continue
                tl = title.lower()
                if any(p in tl for p in bad_phrases):
                    continue
                key = _norm_title(title)
                if key in seen_title:
                    continue
                picked = (title, link)
                break

            if not picked:
                continue

            title, link = picked
            seen_title.add(_norm_title(title))
            items.append((section, title, link))
        except Exception:
            continue

    return items[:limit]

def didi_opening(dt: datetime) -> str:
    wd = dt.weekday()
    date = dt.strftime("%Y-%m-%d")
    day_cn = WEEKDAY_CN[wd]
    day_name = DAY_NAMES[wd]
    mainline = random.choice(MAINLINE_POOL.get(wd, ["å¼Ÿå¼Ÿä»Šå¤©ä¸Šçº¿æ’­æŠ¥å•¦ã€‚"]))
    aside = random.choice(ASIDES)
    return f"ğŸ¾ **{date} Â· ä»Šæ—¥{day_cn}ï¼ˆ{day_name}ï¼‰ï¼**\n{mainline}\n_{aside}_"

def build_card(dt: datetime, weather_tuple, trend_items):
    desc, tmin, tmax, pop = weather_tuple
    opening = didi_opening(dt)

    # å¤©æ°”ä¸€è¡Œæ›´ç´§å‡‘
    weather_line = f"ğŸŒ¤ **æˆéƒ½å¤©æ°”**ï¼š{desc}ï¼Œ{tmin:.0f}â€“{tmax:.0f}Â°Cï½œé™é›¨æ¦‚ç‡ {pop}%"

    elements = [
        {"tag": "div", "text": {"tag": "lark_md", "content": opening}},
        {"tag": "hr"},
        {"tag": "div", "text": {"tag": "lark_md", "content": weather_line}},
        {"tag": "hr"},
    ]

    if trend_items:
        elements.append({
            "tag": "div",
            "text": {"tag": "lark_md", "content": f"ğŸ“° **Trending Newsï¼ˆå¼Ÿå¼Ÿå¼å›æ¥äº† {len(trend_items)} æ¡ï¼‰**"}
        })
        for sec, title, link in trend_items:
            elements.append({
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"- **{sec}**ï¼š[{title}]({link})"}
            })
    else:
        elements.append({
            "tag": "div",
            "text": {"tag": "lark_md", "content": "âš ï¸ ä»Šå¤©æˆ‘å¼æ–°é—»æ—¶è¸©ç©ºäº†â€¦ï¼ˆæºç«™å¯èƒ½æŠ½é£ï¼‰ã€‚æˆ‘æ™šç‚¹å†å»å¼ä¸€è¶Ÿå–µã€‚"}
        })

    # åªä¿ç•™ä¸€ä¸ªæŒ‰é’®
    elements.append({"tag": "hr"})
    elements.append({
        "tag": "action",
        "actions": [
            {
                "tag": "button",
                "text": {"tag": "plain_text", "content": "æ›´å¤šè¶‹åŠ¿ï¼ˆHNï¼‰"},
                "type": "default",
                "url": "https://news.ycombinator.com/",
            }
        ],
    })

    elements.append({
        "tag": "note",
        "elements": [{"tag": "plain_text", "content": "å¼Ÿå¼Ÿå‡ºå“ï½œæ•°æ®æ¥æºï¼šHN / TechCrunch / VentureBeat / Ars Technica"}]
    })

    return {
        "msg_type": "interactive",
        "card": {
            "config": {"wide_screen_mode": True},
            # è¿™é‡Œ header ä¹Ÿå°½é‡çŸ­ï¼Œé¿å…é‡å¤
            # "header": {"title": {"tag": "plain_text", "content": "ğŸ¾ å¼Ÿå¼Ÿ"}},
            "elements": elements,
        },
    }

def send_to_feishu(payload):
    r = requests.post(FEISHU_WEBHOOK, json=payload, timeout=20)
    r.raise_for_status()
    resp = r.json()
    if resp.get("code") != 0:
        raise RuntimeError(resp)
    return resp

def main():
    dt = datetime.now()
    weather = fetch_weather_chengdu()
    trends = fetch_international_trending(limit=3)
    payload = build_card(dt, weather, trends)

    # æœ¬åœ°è°ƒè¯•ï¼šDRY_RUN=1 åªæ‰“å°å¡ç‰‡ JSONï¼Œä¸å‘é€
    if DRY_RUN:
        import json
        print(json.dumps(payload, ensure_ascii=False, indent=2))
        return

    resp = send_to_feishu(payload)
    print("OK", resp)

if __name__ == "__main__":
    main()
